{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subfolder: 240409153123_runs_train_HBI+追加_クリーニング後_２月アノテーション_ランダムsplit__類似性インデックス_0.92, Image count: 0\n",
      "Subfolder: images, Image count: 0\n",
      "Subfolder: train, Image count: 0\n",
      "Subfolder: val, Image count: 0\n",
      "Subfolder: labels, Image count: 0\n",
      "Subfolder: 240409153123_runs_train_HBI+追加_クリーニング後_２月アノテーション_ランダムsplit__類似性インデックス_0.94, Image count: 0\n",
      "Subfolder: 240409153123_runs_train_HBI+追加_クリーニング後_２月アノテーション_ランダムsplit__類似性インデックス_0.96, Image count: 0\n",
      "Subfolder: 240409153123_runs_train_HBI+追加_クリーニング後_２月アノテーション_ランダムsplit__類似性インデックス_0.98, Image count: 0\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.8, Image count: 0\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.82, Image count: 0\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.84, Image count: 0\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.86, Image count: 0\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.88, Image count: 0\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.9, Image count: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace with your root folder path\n",
    "root_folder = r\"D:\\Akash\\Work\\AI\\2025\\TC_1\"\n",
    "\n",
    "# List of valid image extensions\n",
    "valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\", \".webp\"}\n",
    "\n",
    "# Dictionary to store subfolder image counts\n",
    "image_counts = {}\n",
    "\n",
    "# Walk through the root folder\n",
    "for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "    if dirpath != root_folder:  # Skip counting for the root itself\n",
    "        image_count = sum(1 for file in filenames if os.path.splitext(file)[1].lower() in valid_extensions)\n",
    "        subfolder_name = os.path.basename(dirpath)\n",
    "        image_counts[subfolder_name] = image_count\n",
    "\n",
    "# Print results\n",
    "for subfolder, count in image_counts.items():\n",
    "    print(f\"Subfolder: {subfolder}, Image count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subfolder: ., Image count: 0\n",
      "Subfolder: 240409153123_runs_train_HBI+追加_クリーニング後_２月アノテーション_ランダムsplit__類似性インデックス_0.92, Image count: 22657\n",
      "Subfolder: 240409153123_runs_train_HBI+追加_クリーニング後_２月アノテーション_ランダムsplit__類似性インデックス_0.94, Image count: 25399\n",
      "Subfolder: 240409153123_runs_train_HBI+追加_クリーニング後_２月アノテーション_ランダムsplit__類似性インデックス_0.96, Image count: 27818\n",
      "Subfolder: 240409153123_runs_train_HBI+追加_クリーニング後_２月アノテーション_ランダムsplit__類似性インデックス_0.98, Image count: 28979\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.8, Image count: 12578\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.82, Image count: 15283\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.84, Image count: 17929\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.86, Image count: 20568\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.88, Image count: 23346\n",
      "Subfolder: HBI+追加_２月アノテーション_ランダムsplit_類似性インデックス_0.9, Image count: 28065\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace with your root folder path\n",
    "root_folder = r\"D:\\Akash\\Work\\AI\\2025\\TC_1\"\n",
    "\n",
    "# List of valid image extensions\n",
    "valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\", \".webp\"}\n",
    "\n",
    "# Dictionary to store subfolder image counts\n",
    "image_counts = {}\n",
    "\n",
    "# Walk through the root folder\n",
    "for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "    # Extract the top-level subfolder name (relative to the root folder)\n",
    "    relative_path = os.path.relpath(dirpath, root_folder)\n",
    "    top_level_subfolder = relative_path.split(os.sep)[0]  # Get only the immediate subfolder\n",
    "\n",
    "    if top_level_subfolder not in image_counts:\n",
    "        image_counts[top_level_subfolder] = 0\n",
    "\n",
    "    # Count image files in the current directory\n",
    "    image_count = sum(1 for file in filenames if os.path.splitext(file)[1].lower() in valid_extensions)\n",
    "    image_counts[top_level_subfolder] += image_count\n",
    "\n",
    "# Print results\n",
    "for subfolder, count in image_counts.items():\n",
    "    print(f\"Subfolder: {subfolder}, Image count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\trial\\2025_01_tc1_dataset_08\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Set the root folder containing the multiple YOLOv8 datasets\n",
    "root_folder = r\"D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\trial\"\n",
    "\n",
    "# Loop through each subfolder (dataset)\n",
    "for dataset_name in os.listdir(root_folder):\n",
    "    dataset_path = os.path.join(root_folder, dataset_name)\n",
    "\n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(dataset_path):\n",
    "        yaml_file = os.path.join(dataset_path, 'data.yaml')\n",
    "        \n",
    "        # Check if the yaml file exists\n",
    "        if os.path.isfile(yaml_file):\n",
    "            with open(yaml_file, 'r', encoding='utf-8') as file:\n",
    "                # Load the YAML contents\n",
    "                yaml_data = yaml.safe_load(file)\n",
    "\n",
    "            # Modify the YAML data\n",
    "            yaml_data['path'] = os.path.join(root_folder, dataset_name)  # Change 'path' field\n",
    "            # Remove the first comment and add the folder name as a comment\n",
    "            new_comment = f\"# {dataset_name}\"\n",
    "            \n",
    "            # Rewrite the YAML file with the new modifications\n",
    "            with open(yaml_file, 'w', encoding='utf-8') as file:\n",
    "                file.write(new_comment + '\\n')  # Write the new comment\n",
    "                yaml.dump(yaml_data, file, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "            print(f\"Updated {yaml_file}\")\n",
    "\n",
    "        # Delete any classes.txt file found in the dataset folder\n",
    "        classes_txt_path = os.path.join(dataset_path, 'classes.txt')\n",
    "        if os.path.isfile(classes_txt_path):\n",
    "            os.remove(classes_txt_path)\n",
    "            print(f\"Deleted {classes_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\trial\\2025_01_tc1_dataset_08\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Set the root folder containing the multiple YOLOv8 datasets\n",
    "root_folder = r\"D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\trial\"\n",
    "\n",
    "# Loop through each subfolder (dataset)\n",
    "for dataset_name in os.listdir(root_folder):\n",
    "    dataset_path = os.path.join(root_folder, dataset_name)\n",
    "\n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(dataset_path):\n",
    "        yaml_file = os.path.join(dataset_path, 'data.yaml')\n",
    "        \n",
    "        # Check if the yaml file exists\n",
    "        if os.path.isfile(yaml_file):\n",
    "            with open(yaml_file, 'r', encoding='utf-8') as file:\n",
    "                # Load the YAML contents\n",
    "                yaml_data = yaml.safe_load(file)\n",
    "\n",
    "            # Modify the YAML data\n",
    "            yaml_data['path'] = os.path.join(root_folder, dataset_name)  # Change 'path' field\n",
    "            # Remove the first comment and add the folder name as a comment\n",
    "            new_comment = f\"# {dataset_name}\"\n",
    "            \n",
    "            # Rewrite the YAML file with the new modifications\n",
    "            with open(yaml_file, 'w', encoding='utf-8') as file:\n",
    "                file.write(new_comment + '\\n')  # Write the new comment\n",
    "                yaml.dump(yaml_data, file, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "            print(f\"Updated {yaml_file}\")\n",
    "\n",
    "        # Delete any classes.txt file found inside the label subdirectories\n",
    "        labels_folder = os.path.join(dataset_path, 'labels')\n",
    "        if os.path.isdir(labels_folder):\n",
    "            for subdir in os.listdir(labels_folder):\n",
    "                subdir_path = os.path.join(labels_folder, subdir)\n",
    "                # Check if it's a subdirectory\n",
    "                if os.path.isdir(subdir_path):\n",
    "                    classes_txt_path = os.path.join(subdir_path, 'classes.txt')\n",
    "                    if os.path.isfile(classes_txt_path):\n",
    "                        os.remove(classes_txt_path)\n",
    "                        print(f\"Deleted {classes_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_08\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_09\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_10\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_11\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_12\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_13\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_14\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_15\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_16\\data.yaml\n",
      "Updated D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\\2025_01_tc1_dataset_17\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Set the root folder containing the multiple YOLOv8 datasets (dynamically set this)\n",
    "root_folder = r\"D:\\Akash\\Work\\AI\\2025\\TC_1\\datasets\"  # Change this to your actual root folder path\n",
    "\n",
    "# Loop through each subfolder (dataset)\n",
    "for dataset_name in os.listdir(root_folder):\n",
    "    dataset_path = os.path.join(root_folder, dataset_name)\n",
    "\n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(dataset_path):\n",
    "        yaml_file = os.path.join(dataset_path, 'data.yaml')\n",
    "        \n",
    "        # Check if the yaml file exists\n",
    "        if os.path.isfile(yaml_file):\n",
    "            with open(yaml_file, 'r', encoding='utf-8') as file:\n",
    "                # Load the YAML contents\n",
    "                yaml_data = yaml.safe_load(file)\n",
    "\n",
    "            # Modify the YAML data\n",
    "            yaml_data['path'] = os.path.join('/content/datasets', dataset_name).replace(\"\\\\\", \"/\")  # Ensure forward slashes\n",
    "            # Ensure the 'names' field is placed correctly (after 'path', 'train', 'val', 'test')\n",
    "            names = yaml_data.pop('names', None)  # Remove and store 'names' if it exists\n",
    "            yaml_data['names'] = names\n",
    "\n",
    "            # Write the YAML file with the new modifications\n",
    "            with open(yaml_file, 'w', encoding='utf-8') as file:\n",
    "                # Explicitly write the correct order of fields\n",
    "                file.write(f\"path: {yaml_data['path']}\\n\")\n",
    "                file.write(f\"train: {yaml_data['train']}\\n\")\n",
    "                file.write(f\"val: {yaml_data['val']}\\n\")\n",
    "                file.write(f\"test: {yaml_data['test']}\\n\")\n",
    "                file.write(\"names:\\n\")\n",
    "                for key, value in yaml_data['names'].items():\n",
    "                    file.write(f\"  {key}: {value}\\n\")\n",
    "                file.write(f\"\\n#{dataset_name}\\n\")  # Add the comment at the end\n",
    "\n",
    "            print(f\"Updated {yaml_file}\")\n",
    "\n",
    "        # Delete any classes.txt file found inside any folder within the subdirectory\n",
    "        for root_dir, subdirs, files in os.walk(dataset_path):\n",
    "            for file in files:\n",
    "                # Check if it's a classes.txt file\n",
    "                if file == 'classes.txt':\n",
    "                    classes_txt_path = os.path.join(root_dir, file)\n",
    "                    os.remove(classes_txt_path)\n",
    "                    print(f\"Deleted {classes_txt_path}\")\n",
    "\n",
    "                # Check for .json files\n",
    "                if file.endswith('.json'):\n",
    "                    json_file_path = os.path.join(root_dir, file)\n",
    "                    print(f\"Found .json file: {json_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
