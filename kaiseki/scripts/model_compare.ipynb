{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"D:\\Akash\\Work\\AI\\new_test_data_kaiseki\\confidence_F1_curve_V360.xlsx\"  # Replace with the actual file path\n",
    "df = pd.read_excel(file_path, sheet_name=\"v360_test_dataset\")\n",
    "\n",
    "# Print the column names to identify the correct names\n",
    "print(\"Column Names:\", df.columns)\n",
    "\n",
    "# Assuming the first column is for confidence scores and others are models\n",
    "# Replace 'Confidence' if the actual column name is different\n",
    "confidence_column = 'Confidence'  # Adjust based on actual column names\n",
    "confidence_scores = df[confidence_column]\n",
    "\n",
    "# Get the model columns, excluding the confidence column\n",
    "model_columns = df.columns[df.columns != confidence_column]\n",
    "\n",
    "# Initialize a dictionary to store AUC values\n",
    "auc_scores = {}\n",
    "\n",
    "# Calculate AUC for each model\n",
    "for model in model_columns:\n",
    "    # Extract model predictions\n",
    "    model_predictions = df[model]\n",
    "    \n",
    "    # Ensure that the confidence values and model predictions are valid for AUC calculation\n",
    "    if len(confidence_scores.unique()) > 1:\n",
    "        # Compute the AUC using the confidence scores and model predictions\n",
    "        auc = roc_auc_score(confidence_scores, model_predictions)\n",
    "        auc_scores[model] = auc\n",
    "    else:\n",
    "        auc_scores[model] = None  # Handle case where AUC cannot be computed\n",
    "\n",
    "# Convert the AUC scores to a DataFrame for better display\n",
    "auc_df = pd.DataFrame(list(auc_scores.items()), columns=['Model', 'AUC'])\n",
    "\n",
    "# Display the AUC table\n",
    "print(auc_df)\n",
    "\n",
    "# Optionally save the AUC scores to an Excel file\n",
    "output_file_path = r\"D:\\Akash\\Work\\AI\\new_test_data_kaiseki\\analysis\\auc_scores.xlsx\"  # Replace with your desired output path\n",
    "auc_df.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Model       AUC\n",
      "0                                           yolov5s  0.244085\n",
      "1                                           yolov5m  0.249020\n",
      "2                                           yolov5l  0.259974\n",
      "3                                           yolov5x  0.275669\n",
      "4              231216091234_runs_train_HBI_epoch100  0.520860\n",
      "..                                              ...       ...\n",
      "176  240918073603_runs_train_2024_09_tc4_dataset_13  0.601273\n",
      "177   240922184434_runs_train_2024_09_tc4_dataset_1  0.590577\n",
      "178   240922185231_runs_train_2024_09_tc4_dataset_3  0.583300\n",
      "179   240922192458_runs_train_2024_09_tc4_dataset_2  0.590101\n",
      "180   240923071239_runs_train_2024_09_tc4_dataset_4  0.588668\n",
      "\n",
      "[181 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file and specify the sheet name\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=\"v360_test_dataset\")\n",
    "\n",
    "# Extract confidence scores from column A\n",
    "confidence_scores = df.iloc[:, 0]  # Assuming the first column (A) contains the confidence values\n",
    "\n",
    "# Extract model outputs from column B onwards\n",
    "model_columns = df.columns[1:]  # Columns B and onwards represent model outputs\n",
    "\n",
    "# Initialize a dictionary to store AUC values\n",
    "auc_scores = {}\n",
    "\n",
    "# Iterate through each model and calculate AUC using np.trapz\n",
    "for model in model_columns:\n",
    "    # Extract model predictions\n",
    "    model_predictions = df[model]\n",
    "    \n",
    "    # Combine confidence and predictions into a temporary DataFrame\n",
    "    temp_df = pd.DataFrame({'confidence': confidence_scores, 'predictions': model_predictions})\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    temp_df = temp_df.dropna()\n",
    "\n",
    "    # Sort values by confidence (x-axis)\n",
    "    temp_df = temp_df.sort_values(by='confidence')\n",
    "    \n",
    "    # Use np.trapz to compute the area under the curve (AUC)\n",
    "    auc = np.trapz(temp_df['predictions'], x=temp_df['confidence'])\n",
    "    \n",
    "    # Store the AUC for this model\n",
    "    auc_scores[model] = auc\n",
    "\n",
    "# Convert the AUC scores to a DataFrame for better display\n",
    "auc_df = pd.DataFrame(list(auc_scores.items()), columns=['Model', 'AUC'])\n",
    "\n",
    "# Display the AUC table\n",
    "print(auc_df)\n",
    "\n",
    "# Optionally save the AUC scores to an Excel file\n",
    "auc_df.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Model       AUC\n",
      "88   240704101619_runs_train_202403_7_BGs_2percent_ex1  0.700050\n",
      "123     240727182157_runs_train_2024_07_tc1_dataset_26  0.621201\n",
      "139      240803061121_runs_train_2024_07_tc1_dataset_4  0.621019\n",
      "142      240803175810_runs_train_2024_07_tc1_dataset_7  0.620926\n",
      "161     240901083931_runs_train_2024_08_tc3_dataset_16  0.615250\n",
      "..                                                 ...       ...\n",
      "2                                              yolov5l  0.259974\n",
      "1                                              yolov5m  0.249020\n",
      "0                                              yolov5s  0.244085\n",
      "49            240524153431_runs_train_fisheye_overhead  0.217369\n",
      "51                     240524182726_runs_train_WEPDTOF  0.199290\n",
      "\n",
      "[181 rows x 2 columns]\n",
      "AUC scores saved to D:\\Akash\\Work\\AI\\new_test_data_kaiseki\\analysis\\auc_scores_trapz_sorted.xlsx with conditional color coding.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.formatting.rule import ColorScaleRule\n",
    "\n",
    "# Load the Excel file and specify the sheet name\n",
    "# file_path = \"/mnt/data/your_file.xlsx\"  # Replace with actual file path\n",
    "df = pd.read_excel(file_path, sheet_name=\"v360_test_dataset\")\n",
    "\n",
    "# Extract confidence scores from column A\n",
    "confidence_scores = df.iloc[:, 0]  # Assuming the first column (A) contains the confidence values\n",
    "\n",
    "# Extract model outputs from column B onwards\n",
    "model_columns = df.columns[1:]  # Columns B and onwards represent model outputs\n",
    "\n",
    "# Initialize a dictionary to store AUC values\n",
    "auc_scores = {}\n",
    "\n",
    "# Iterate through each model and calculate AUC using np.trapz\n",
    "for model in model_columns:\n",
    "    # Extract model predictions\n",
    "    model_predictions = df[model]\n",
    "    \n",
    "    # Combine confidence and predictions into a temporary DataFrame\n",
    "    temp_df = pd.DataFrame({'confidence': confidence_scores, 'predictions': model_predictions})\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    temp_df = temp_df.dropna()\n",
    "\n",
    "    # Sort values by confidence (x-axis)\n",
    "    temp_df = temp_df.sort_values(by='confidence')\n",
    "    \n",
    "    # Use np.trapz to compute the area under the curve (AUC)\n",
    "    auc = np.trapz(temp_df['predictions'], x=temp_df['confidence'])\n",
    "    \n",
    "    # Store the AUC for this model\n",
    "    auc_scores[model] = auc\n",
    "\n",
    "# Convert the AUC scores to a DataFrame for better display\n",
    "auc_df = pd.DataFrame(list(auc_scores.items()), columns=['Model', 'AUC'])\n",
    "\n",
    "# Sort the AUC scores in descending order\n",
    "auc_df = auc_df.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "# Display the sorted AUC table\n",
    "print(auc_df)\n",
    "\n",
    "# Save the AUC scores to an Excel file\n",
    "output_file_path = r\"D:\\Akash\\Work\\AI\\new_test_data_kaiseki\\analysis\\auc_scores_trapz_sorted.xlsx\"\n",
    "auc_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Apply conditional color coding using openpyxl\n",
    "wb = load_workbook(output_file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Define a color scale (green for high AUC, red for low AUC)\n",
    "color_rule = ColorScaleRule(\n",
    "    start_type=\"percentile\", start_value=0, start_color=\"FF6347\",  # Red for low AUC\n",
    "    mid_type=\"percentile\", mid_value=50, mid_color=\"FFFF00\",  # Yellow for mid AUC\n",
    "    end_type=\"percentile\", end_value=100, end_color=\"32CD32\"  # Green for high AUC\n",
    ")\n",
    "\n",
    "# Apply the color rule to the AUC column\n",
    "ws.conditional_formatting.add('B2:B{}'.format(len(auc_df)+1), color_rule)\n",
    "\n",
    "# Save the workbook with color formatting\n",
    "wb.save(output_file_path)\n",
    "\n",
    "print(f\"AUC scores saved to {output_file_path} with conditional color coding.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
